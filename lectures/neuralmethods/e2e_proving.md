# E2E Differential Proving
NTP = neural theorem provers
Constructing And Or, Substition Networks
Chained with Prolog backward chaining algorithm

Two ways of dealing with unknown relations
- neural links - vector representation of relations and variables
  - good generalizability, can easily transfer to unseen constants
  - bad inference

- direct logic / inductive logic programming
  - implemented in prolog
  - good inference
  - bad - hard to infer
  - compability

New way: neural prolog

- new approach: neural prolog
  - e2e training, logic rules are not integrated into the vectors but part of the structure
  - possible through differentiable function values
  - logical blocks make for interpretable connections
  - neural components allow for representation and abstraction

Interpretability
- rules exist in logic but not in neural networks
  - in NTP rules exist as complete neural networks
- logic 
- 

Intersection
- extend beyond embeddings and add logical rules
  - before that: background paper does not concern with logical formulae
  - father / mother relationships can be similar
    - marge / maggie vs homer / bart can be abstracted
    - It's hard to get from Abe to bart because things can not be chained
  - do logical rules get used 

Logical inference in matrix stuff
- Pre = generate more training data
- Joint = enforce matrix training
- Post = fill up test results

TODO
- check if reasons for cell decision can be inspected
- understand the cell parameters as matrix factorization
- demonstrate why inference why inference works for logic but not for neural prediction models
- read viz-papers
- check back with frank

Term-Paper



Question
- why function-free
- difference to other systems
- how does the intrepretability work
- intersection between papers
